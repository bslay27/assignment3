{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4f0c98-025b-4310-af80-3190e7a6754e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Outcome'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     24\u001b[0m dataset\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m###########################\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Preprocessing\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m###########################\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m Y \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutcome\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Spliting dataset into Training Data and Test Data with an 80% and 20% split between training and evaluation.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Training data is used to train our Logistic model.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Test data will be used to validate our model.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5345\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5346\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5347\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5348\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5349\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5350\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5351\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5352\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Outcome'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "####################################################################\n",
    "\n",
    "###########################\n",
    "# Importing Dataset\n",
    "###########################\n",
    "\n",
    "dataset = pd.read_csv('/Users/benjaminslay/Downloads/diabetes.csv')\n",
    "dataset.head()\n",
    "\n",
    "###########################\n",
    "# Preprocessing\n",
    "###########################\n",
    "\n",
    "X = dataset.drop('Outcome', axis=1)\n",
    "Y = dataset['Outcome']\n",
    "\n",
    "# Spliting dataset into Training Data and Test Data with an 80% and 20% split between training and evaluation.\n",
    "# Training data is used to train our Logistic model.\n",
    "# Test data will be used to validate our model.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling to scale data between 0 and 1 to get better accuracy.\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Make an instance classifier with 1000 max iterations and random state of 0\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=0)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "###########################\n",
    "# Results\n",
    "###########################\n",
    "\n",
    "# Predict the Test set results\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model using model evaluation metrics accuracy, precision, recall, and F1 score\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(Y_test, Y_pred))\n",
    "\n",
    "###########################\n",
    "# Training Results\n",
    "###########################\n",
    "\n",
    "# Number of iterations\n",
    "accuracy = []\n",
    "iterations = classifier.n_iter_[0]\n",
    "loss = []\n",
    "\n",
    "for i in range(1, iterations + 1):\n",
    "    classifier = LogisticRegression(solver='liblinear', max_iter=i, random_state=0)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_train_prediction = classifier.predict(X_train)\n",
    "    accuracy.append(metrics.accuracy_score(Y_train, Y_train_prediction))\n",
    "    loss.append(-classifier.score(X_train, Y_train))\n",
    "\n",
    "###########################\n",
    "# Plotting Training Results\n",
    "###########################\n",
    "\n",
    "fig, axis1 = plt.subplots()\n",
    "\n",
    "axis2 = axis1.twinx()\n",
    "axis1.plot(range(1, iterations + 1), loss, 'r-')\n",
    "axis2.plot(range(1, iterations + 1), accuracy, 'g-')\n",
    "\n",
    "axis1.set_xlabel('Iteration')\n",
    "axis1.set_ylabel('Loss', color='r')\n",
    "axis2.set_ylabel('Accuracy', color='g')\n",
    "\n",
    "plt.title('Training Results - Accuracy and Loss over Iterations')\n",
    "\n",
    "###########################\n",
    "# Confusion Matrix\n",
    "###########################\n",
    "\n",
    "cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, fmt='d', cmap='summer', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Problem 2 - Part 1\n",
    "\n",
    "####################################################################\n",
    "\n",
    "###########################\n",
    "# Importing new Dataset\n",
    "###########################\n",
    "\n",
    "dataset = pd.read_csv('/Users/benjaminslay/Downloads/cancer.csv')\n",
    "dataset.head()\n",
    "\n",
    "###########################\n",
    "# Preprocessing\n",
    "###########################\n",
    "\n",
    "# Cleaning data\n",
    "# Drop the 'Unnamed: 32' column\n",
    "dataset.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)\n",
    "\n",
    "# Mapping M and B values of the diagnosis to 1 and 0\n",
    "dataset['diagnosis'] = dataset['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "X = dataset.drop('diagnosis', axis=1)\n",
    "Y = dataset['diagnosis']\n",
    "\n",
    "# Spliting dataset into Training Data and Test Data with an 80% and 20% split between training and evaluation.\n",
    "# Training data is used to train our Logistic model.\n",
    "# Test data will be used to validate our model.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling to scale data between 0 and 1 to get better accuracy.\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Make an instance classifier for a linear regression with 1000 max iterations and random state of 0\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=0)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "###########################\n",
    "# Results\n",
    "###########################\n",
    "\n",
    "# Predict the Test set results\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model using model evaluation metrics accuracy, precision, recall, and F1 score\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(Y_test, Y_pred))\n",
    "\n",
    "###########################\n",
    "# Training Results\n",
    "###########################\n",
    "\n",
    "accuracy = []\n",
    "iterations = classifier.n_iter_[0]\n",
    "loss = []\n",
    "\n",
    "for i in range(1, iterations + 1):\n",
    "    classifier = LogisticRegression(solver='liblinear', max_iter=i, random_state=0)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_train_prediction = classifier.predict(X_train)\n",
    "    accuracy.append(metrics.accuracy_score(Y_train, Y_train_prediction))\n",
    "    loss.append(-classifier.score(X_train, Y_train))\n",
    "\n",
    "###########################\n",
    "# Plotting Training Results\n",
    "###########################\n",
    "\n",
    "fig, axis1 = plt.subplots()\n",
    "\n",
    "axis2 = axis1.twinx()\n",
    "axis1.plot(range(1, iterations + 1), loss, 'r-')\n",
    "axis2.plot(range(1, iterations + 1), accuracy, 'g-')\n",
    "\n",
    "axis1.set_xlabel('Iteration')\n",
    "axis1.set_ylabel('Loss', color='r')\n",
    "axis2.set_ylabel('Accuracy', color='g')\n",
    "\n",
    "plt.title('Training Results - Accuracy and Loss over Iterations')\n",
    "\n",
    "###########################\n",
    "# Confusion Matrix\n",
    "###########################\n",
    "\n",
    "cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, fmt='d', cmap='summer', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Problem 2 - Part 2 - Weight Penalty\n",
    "\n",
    "####################################################################\n",
    "\n",
    "classifier_l2 = LogisticRegression(max_iter=1000, penalty = 'l2', C=1.0, random_state=0)\n",
    "classifier_l2.fit(X_train, Y_train)\n",
    "\n",
    "###########################\n",
    "# Results\n",
    "###########################\n",
    "\n",
    "# Predict the Test set results\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model using model evaluation metrics accuracy, precision, recall, and F1 score\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(Y_test, Y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(Y_test, Y_pred))\n",
    "\n",
    "###########################\n",
    "# Training Results\n",
    "###########################\n",
    "\n",
    "accuracy = []\n",
    "iterations = classifier.n_iter_[0]\n",
    "loss = []\n",
    "\n",
    "for i in range(1, iterations + 1):\n",
    "    classifier = LogisticRegression(solver='liblinear', penalty='l2', C=1.0, max_iter=i, random_state=42)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_train_prediction = classifier.predict(X_train)\n",
    "    accuracy.append(metrics.accuracy_score(Y_train, Y_train_prediction))\n",
    "    loss.append(-classifier.score(X_train, Y_train))\n",
    "\n",
    "###########################\n",
    "# Plotting Training Results\n",
    "###########################\n",
    "\n",
    "fig, axis1 = plt.subplots()\n",
    "\n",
    "axis2 = axis1.twinx()\n",
    "axis1.plot(range(1, iterations + 1), loss, 'r-')\n",
    "axis2.plot(range(1, iterations + 1), accuracy, 'g-')\n",
    "\n",
    "axis1.set_xlabel('Iteration')\n",
    "axis1.set_ylabel('Loss', color='r')\n",
    "axis2.set_ylabel('Accuracy', color='g')\n",
    "\n",
    "plt.title('Training Results - Accuracy and Loss over Iterations')\n",
    "\n",
    "###########################\n",
    "# Confusion Matrix\n",
    "###########################\n",
    "\n",
    "# Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "#Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn.\n",
    "#Here, you will visualize the confusion matrix using Heatmap.\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, fmt='d', cmap='summer', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Problem 3\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Spliting dataset into Training Data and Test Data with an 80% and 20% split between training and evaluation.\n",
    "# Training data is used to train our Logistic model.\n",
    "# Test data will be used to validate our model.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling to scale data between 0 and 1 to get better accuracy.\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "# Make an instance classifier for a Naive Bayes\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "##################\n",
    "# Results\n",
    "##################\n",
    "\n",
    "# Predict the Test set results\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model using model evaluation metrics accuracy, precision, recall, and F1 score\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision = metrics.precision_score(Y_test, Y_pred)\n",
    "recall = metrics.recall_score(Y_test, Y_pred)\n",
    "f1_score = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"F1 Score:\",f1_score)\n",
    "\n",
    "# Plot the evaluation metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "values = [accuracy, precision, recall, f1_score]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Naive Bayes Classifier Performance Metrics')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Problem 4\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Spliting dataset into Training Data and Test Data with an 80% and 20% split between training and evaluation.\n",
    "# Training data is used to train our Logistic model.\n",
    "# Test data will be used to validate our model.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling to scale data between 0 and 1 to get better accuracy.\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1_score = 0\n",
    "\n",
    "K_iterations = range(1, X_train.shape[1] + 1)\n",
    "\n",
    "for k in K_iterations:\n",
    "    pca = PCA(n_components=k)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    classifier = LogisticRegression(max_iter=1000, random_state=0)\n",
    "    classifier.fit(X_train_pca, Y_train)\n",
    "\n",
    "    Y_pred = classifier.predict(X_test_pca)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    precision = metrics.precision_score(Y_test, Y_pred)\n",
    "    recall = metrics.recall_score(Y_test, Y_pred)\n",
    "    f1_score = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_score_list.append(f1_score)\n",
    "\n",
    "K_value = K_iterations[np.argmax(np.array([accuracy, precision, recall, f1_score]))]\n",
    "\n",
    "print(\"K =\", K_value)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print()\n",
    "\n",
    "# Plot the evaluation metrics over different values of K\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.plot(K_iterations, accuracy_list, label='Accuracy', marker='o')\n",
    "plt.plot(K_iterations, precision_list, label='Precision', marker='o')\n",
    "plt.plot(K_iterations, recall_list, label='Recall', marker='o')\n",
    "plt.plot(K_iterations, f1_score_list, label='F1 Score', marker='o')\n",
    "\n",
    "plt.xlabel('Number of Principal Components (K)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Evaluation Metrics vs. Number of Principal Components (K)')\n",
    "plt.grid(True)\n",
    "plt.xticks(K_iterations)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Problem 5\n",
    "\n",
    "####################################################################\n",
    "\n",
    "# Spliting dataset into Training Data and Test Data with an 80% and 20% split between training and evaluation.\n",
    "# Training data is used to train our Logistic model.\n",
    "# Test data will be used to validate our model.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature scaling to scale data between 0 and 1 to get better accuracy.\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1_score = 0\n",
    "\n",
    "K_iterations = range(1, X_train.shape[1] + 1)\n",
    "\n",
    "for k in K_iterations:\n",
    "    pca = PCA(n_components=k)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train_pca, Y_train)\n",
    "\n",
    "    Y_pred = classifier.predict(X_test_pca)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    precision = metrics.precision_score(Y_test, Y_pred)\n",
    "    recall = metrics.recall_score(Y_test, Y_pred)\n",
    "    f1_score = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_score_list.append(f1_score)\n",
    "\n",
    "K_value = K_iterations[np.argmax(np.array([accuracy, precision, recall, f1_score]))]\n",
    "\n",
    "print(\"K =\", K_value)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print()\n",
    "\n",
    "# Plot the evaluation metrics over different values of K\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "plt.plot(K_iterations, accuracy_list, label='Accuracy', marker='o')\n",
    "plt.plot(K_iterations, precision_list, label='Precision', marker='o')\n",
    "plt.plot(K_iterations, recall_list, label='Recall', marker='o')\n",
    "plt.plot(K_iterations, f1_score_list, label='F1 Score', marker='o')\n",
    "\n",
    "plt.xlabel('Number of Principal Components (K)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Evaluation Metrics vs. Number of Principal Components (K)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(K_iterations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bf973-8485-459a-a5fa-f0043a801d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
